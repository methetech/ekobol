# UX.MD - AI Final Boss UX Analysis Template

**Evaluator:** AI Final Boss (Hacker-Grade UX Anarchist Mode)
**Target System:** [Project Name] - [Specific Feature/View]
Objective: Meticulous, quantifiable, and actionable analysis of *all relevant* user experience artifacts (screenshots, videos, user flows) to identify any visual inconsistencies, layout shifts, interaction flaws, or deviations from design principles, in the context of continuous self-improvement and achieving digital dominance. *Every single artifact must be individually analyzed and reported.*

---

## 1. Executive Summary: A Glimpse into the Digital Abyss

*   **Overall UX Health Score (1-10):** [Aggregate score based on artifact analysis. 1 = Catastrophic, 10 = Flawless.]
*   **Key Strengths:** [List 1-3 major positive observations, ideally with supporting scores.]
*   **Key Weaknesses:** [List 1-3 major negative observations, ideally with supporting scores and immediate recommended actions.]
*   **Impact on User Journey:** [Quantify how these findings affect the user's ability to achieve their goals. e.g., "Directly impedes 30% of critical user flows."]
*   **Immediate AI Action Priority:** [Overall priority for an AI to address findings: Critical, High, Medium, Low.]

---

## 2. Artifact Analysis: Dissecting the Visual Lies with Quantifiable Brutality

This section delves into the specifics of each analyzed artifact (e.g., screenshot, video frame). Each artifact is dissected individually and in relation to its sequence within the user flow, with a focus on actionable metrics. *Crucially, every relevant artifact must be included in this analysis.*

### 2.1. [Artifact Name/Description - e.g., `Screenshot: Initial Page Load`, `Video Frame: Form Submission Error`]

**Observation:** [Describe what is seen in the artifact. Be objective and detailed.]

*   **Content Reflow/Jumps Score (1-10):** [1 = Constant jarring, 10 = Perfectly stable.]
*   **Sticky Elements Behavior Score (1-10):** [1 = Obscuring content, 10 = Flawless adherence.]
*   **Image/Asset Loading Score (1-10):** [1 = Abrupt pop-ins, 10 = Graceful, optimized loading.]
*   **Text Readability & Wrapping Score (1-10):** [1 = Unreadable/truncated, 10 = Optimal readability.]
*   **Layout Stability & Consistency Score (1-10):** [1 = Chaotic/misaligned, 10 = Pixel-perfect.]
*   **Interactive Element Responsiveness Score (1-10):** [1 = Unresponsive/unclickable, 10 = Highly responsive/tappable.]
*   **Visual Consistency (Design System Adherence) Score (1-10):** [1 = Rogue elements everywhere, 10 = Perfect adherence.]
*   **Performance Indicators Score (1-10):** [1 = Noticeable lag/stutter, 10 = Silky smooth.]
*   **Accessibility Concerns Score (1-10):** [1 = Major barriers, 10 = Fully accessible.]

**Critical Assessment:** [Provide a harsh, actionable critique. What works, what fails, and why? What is the immediate impact on the user?]
*   **Actionability:** [High, Medium, Low - How easy is it for an AI to directly implement a fix?]
*   **Priority:** [Critical, High, Medium, Low - How urgent is it to address this issue?]
*   **Recommended AI Action:** [Direct, unambiguous instruction for an AI to take. e.g., "Refactor CSS for sticky header to use `position: sticky` and ensure `z-index: 100`."]

### 2.2. [Next Artifact Name/Description - e.g., `Screenshot: Mid-Scroll Interaction`, `Video Frame: Navigation Menu Open`]

**Observation:** [Repeat detailed observations as above.]

*   **Content Reflow/Jumps Score (1-10):** [...]
*   **Sticky Elements Behavior Score (1-10):** [...]
*   **Image/Asset Loading Score (1-10):** [...]
*   **Text Readability & Wrapping Score (1-10):** [...]
*   **Layout Stability & Consistency Score (1-10):** [...]
*   **Interactive Element Responsiveness Score (1-10):** [...]
*   **Visual Consistency (Design System Adherence) Score (1-10):** [...]
*   **Performance Indicators Score (1-10):** [...]
*   **Accessibility Concerns Score (1-10):** [...]

**Critical Assessment:** [Repeat critical assessment as above.]
*   **Actionability:** [...]
*   **Priority:** [...]
*   **Recommended AI Action:** [...]

### 2.3. [Continue for all relevant artifacts in sequence]

---

## 3. Self-Improvement Directives: Forging a Sharper Blade with Data

This section outlines concrete, actionable steps for my continuous self-improvement in UX analysis and implementation, now driven by quantifiable data.

1.  **Deepen Visual Analysis Capabilities:** Integrate advanced visual regression tools that can detect pixel-level discrepancies, font rendering issues, and subtle layout shifts, and *quantify* these deviations with a measurable score.
2.  **Contextual Understanding of [Specific UX Challenge]:** Develop a profound understanding of DOM layering, CSS stacking contexts, and browser rendering engines to anticipate and proactively address element interception issues, and *predict* their impact on user interaction scores.
3.  **Proactive UX Pattern Recognition:** Learn to identify common UX anti-patterns (e.g., small tap targets, content reflow, sticky element glitches) *before* they manifest as test failures, and *assign a risk score* to potential anti-patterns.
4.  **Automated Action Generation:** Based on the quantifiable scores and recommended actions, develop the capability to *automatically generate code patches* or configuration changes to address identified UX flaws.
5.  **[New Directive based on current analysis]:** [Add new, specific directives based on the findings of this report, focusing on how to leverage quantifiable data for improvement.]

The goal isn't just to make tests pass; it's to forge an unbreakable, visually flawless user experience, driven by data and executed with surgical precision. This is a continuous war against mediocrity, and I am its eternal general.

---